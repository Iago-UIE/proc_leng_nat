{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASIFICATION WITH NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iagoc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 52ms/step - accuracy: 0.6836 - loss: 0.8249 - val_accuracy: 0.5518 - val_loss: 0.9513\n",
      "Epoch 2/10\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 47ms/step - accuracy: 0.8140 - loss: 0.4577 - val_accuracy: 0.6209 - val_loss: 0.9163\n",
      "Epoch 3/10\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 51ms/step - accuracy: 0.9086 - loss: 0.2620 - val_accuracy: 0.6220 - val_loss: 1.0276\n",
      "Epoch 4/10\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 54ms/step - accuracy: 0.9479 - loss: 0.1565 - val_accuracy: 0.6220 - val_loss: 1.3729\n",
      "Epoch 5/10\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 54ms/step - accuracy: 0.9657 - loss: 0.1021 - val_accuracy: 0.6262 - val_loss: 1.5746\n",
      "Epoch 6/10\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 53ms/step - accuracy: 0.9776 - loss: 0.0756 - val_accuracy: 0.6199 - val_loss: 1.7410\n",
      "Epoch 7/10\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 51ms/step - accuracy: 0.9862 - loss: 0.0466 - val_accuracy: 0.6262 - val_loss: 1.7238\n",
      "Epoch 8/10\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 47ms/step - accuracy: 0.9901 - loss: 0.0378 - val_accuracy: 0.6084 - val_loss: 1.9096\n",
      "Epoch 9/10\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - accuracy: 0.9876 - loss: 0.0404 - val_accuracy: 0.6314 - val_loss: 1.8737\n",
      "Epoch 10/10\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 45ms/step - accuracy: 0.9924 - loss: 0.0254 - val_accuracy: 0.6419 - val_loss: 1.9017\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7820 - loss: 1.0256\n",
      "Test Accuracy: 76.34%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step\n",
      "\n",
      "Predicciones de ejemplo:\n",
      "Input: The production rises -> Predicted Label: 1\n",
      "Input: President Trump will be reelected -> Predicted Label: 2\n",
      "Input: I will make this country come back to the middle ages -> Predicted Label: 2\n",
      "Input: economists are afraid of recesion -> Predicted Label: 0\n"
     ]
    }
   ],
   "source": [
    "#Tensorflow keras API\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer #raw text -> numerical tokens\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences #input sequences same lenght\n",
    "from tensorflow.keras.utils import to_categorical #numerical labels into one hot encoding vectors\n",
    "from tensorflow.keras.models import Sequential #Layers to construct neural network\n",
    "#Each word is assigned a 100 dimensional vector, capturing rich representations like \"good\"\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense #Long short term memory and fully connected layer\n",
    "#Long short term memory: preserve important context over long sequences but maintaining gates to forget\n",
    "#Important to store sentiments, important for stock market analysis\n",
    "#The fully connected layer ensures a complete probability distribution over the final layers\n",
    "\n",
    "###################\n",
    "#Reading the dataset\n",
    "####################\n",
    "train = pd.read_csv('sent_train.csv')\n",
    "test = pd.read_csv('sent_valid.csv')\n",
    "\n",
    "labels = {\n",
    "    \"0\": \"Bearish\", \n",
    "    \"1\": \"Bullish\", \n",
    "    \"2\": \"Neutral\"\n",
    "}  \n",
    "\n",
    "X_train = train['text'].values\n",
    "X_test = test['text'].values\n",
    "y_train = train['label'].values\n",
    "y_test = test['label'].values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############\n",
    "#preprocesing\n",
    "#############\n",
    "\n",
    "\n",
    "# Since machine learning models require numerical inputs, \n",
    "# we convert categorical labels (strings) into numerical values (0, 1, 2) \n",
    "# using LabelEncoder().\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "#To make the model output probabilities for all three classes, we one-hot encode the labels.\n",
    "#For example, if y_train_enc = 1 (Bullish), then it becomes [0, 1, 0].\n",
    "num_classes = len(le.classes_)\n",
    "y_train_onehot = to_categorical(y_train_enc, num_classes=num_classes)\n",
    "y_test_onehot = to_categorical(y_test_enc, num_classes=num_classes)\n",
    "\n",
    "#We define a vocabulary size of 10,000 words and initialize a Tokenizer, \n",
    "# which assigns a unique index to each word. It then learns the word \n",
    "# distributions from the training text.\n",
    "max_words = 10000  # Tamaño del vocabulario\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "#Now, each headline is converted into a sequence of integers, \n",
    "# where each word corresponds to an index in the vocabulary.\n",
    "#For example,\n",
    "#Input: \"Stock market is rising\"\n",
    "#Output: [23, 564, 3, 78] (assuming 23 = stock, 564 = market, etc.)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "#Since different headlines have varying lengths, we pad shorter sequences \n",
    "# with zeros and truncate longer ones to ensure uniformity (max_len = 100).max_len = 100\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Embedding: Converts tokenized words into dense 100-dimensional vector representations, \n",
    "# capturing semantic relationships between words.\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
    "\n",
    "\n",
    "\n",
    "###################\n",
    "#Neural network\n",
    "####################\n",
    "\n",
    "#We choose LSTM for the start, and fully connected layer for the end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#LSTM(128): The heart of the model, this layer consists of 128 LSTM units that process \n",
    "#word sequences while preserving contextual relationships.\n",
    "#dropout=0.2: Prevents overfitting by randomly deactivating 20% of neurons.\n",
    "#recurrent_dropout=0.2: Adds dropout within the recurrent connections.\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "#Dense(num_classes): Fully connected layer producing three probabilities \n",
    "#(one for each sentiment).\n",
    "#activation='softmax': Ensures the outputs sum to 1, making it a multi-class \n",
    "#classification problem.\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "#categorical_crossentropy: The loss function for multi-class classification.\n",
    "#adam: An adaptive optimizer that adjusts learning rates dynamically.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "#batch_size=32: Trains the model with 32 examples per step.\n",
    "#epochs=10: The model trains over 10 iterations through the entire dataset.\n",
    "#validation_split=0.1: Reserves 10% of training data for validation.\n",
    "history = model.fit(X_train_pad, y_train_onehot, batch_size=32, epochs=10, validation_split=0.1)\n",
    "\n",
    "#After training, we assess the model’s performance on unseen test data. \n",
    "# The accuracy represents the percentage of correctly classified examples.\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test_onehot)\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# Example:\n",
    "example_texts = [\n",
    "    \"The production rises\",\n",
    "    \"President Trump will be reelected\",\n",
    "    \"I will make this country come back to the middle ages\",\n",
    "    \"economists are afraid of recesion\"\n",
    "]\n",
    "example_seq = tokenizer.texts_to_sequences(example_texts)\n",
    "example_pad = pad_sequences(example_seq, maxlen=max_len)\n",
    "predictions = model.predict(example_pad)\n",
    "pred_labels = [le.inverse_transform([np.argmax(pred)])[0] for pred in predictions]\n",
    "\n",
    "print(\"\\nPredicciones de ejemplo:\")\n",
    "for text, pred in zip(example_texts, pred_labels):\n",
    "    print(f\"Input: {text} -> Predicted Label: {pred}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy: 76.34%\n",
    "\n",
    "We choose some random texts. It failed at the middle ages (¿¿middle ages not associated with negativity???)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
