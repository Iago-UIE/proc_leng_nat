{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\iagoc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iagoc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iagoc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted sentences:  ['The', 'president', 'of', 'the', 'U.S.A.', ',', 'Donald', 'Trump', ',', 'is', '1.9m', 'high', 'and', '78', 'years', 'old', '.', 'Forbes', 'Magazine', 'has', 'assessed', 'his', 'wealth', ',', 'currently', 'estimating', 'it', 'at', '$', '5.5', 'billion', 'as', 'of', 'mid-February', '2025', '.']\n",
      "\n",
      "The converted text is:  The president of the USA, Donald Trump, is oneninezero centimeters high and seveneight years old Forbes Magazine has assessed his wealth, currently estimating it at five point five billion as of mid-February twozerotwofive point \n",
      "\n",
      "Lower case is:  the president of the u.s.a. , donald trump , is 1.9m high and 78 years old. forbes magazine has assessed his wealth , currently estimating it at $ 5.5 billion as of mid-february 2025 .\n",
      "\n",
      "Tokens are:  ['The', 'president', 'of', 'the', 'U.S.A.', ',', 'Donald', 'Trump', ',', 'is', '1.9m', 'high', 'and', '78', 'years', 'old.', 'Forbes', 'Magazine', 'has', 'assessed', 'his', 'wealth', ',', 'currently', 'estimating', 'it', 'at', '$', '5.5', 'billion', 'as', 'of', 'mid-February', '2025', '.']\n",
      "\n",
      "Text without stopwords:  The president U.S.A. , Donald Trump , 1.9m high 78 years old . Forbes Magazine assessed wealth , currently estimating $ 5.5 billion mid-February 2025 .\n",
      "\n",
      "Bigrams: [('The', 'president'), ('president', 'of'), ('of', 'the'), ('the', 'USA'), ('USA', ','), (',', 'Donald'), ('Donald', 'Trump'), ('Trump', ','), (',', 'is'), ('is', 'oneninezero'), ('oneninezero', 'centimeters'), ('centimeters', 'high'), ('high', 'and'), ('and', 'seveneight'), ('seveneight', 'years'), ('years', 'old'), ('old', 'Forbes'), ('Forbes', 'Magazine'), ('Magazine', 'has'), ('has', 'assessed'), ('assessed', 'his'), ('his', 'wealth'), ('wealth', ','), (',', 'currently'), ('currently', 'estimating'), ('estimating', 'it'), ('it', 'at'), ('at', 'five'), ('five', 'point'), ('point', 'five'), ('five', 'billion'), ('billion', 'as'), ('as', 'of'), ('of', 'mid-February'), ('mid-February', 'twozerotwofive'), ('twozerotwofive', 'point')]\n",
      "Tokens: ['The', 'president', 'president', 'president', 'is', 'is', 'after', 'president', 'after', 'president']\n",
      "\n",
      "Bigram model (each word and the words that follow it with counts):\n",
      "The -> {'president': 1}\n",
      "president -> {'president': 2, 'is': 1, 'after': 1}\n",
      "is -> {'is': 1, 'after': 1}\n",
      "after -> {'president': 2}\n",
      "\n",
      "Predicted word after 'USA,': None\n",
      "\n",
      "Predicted word after 'Donald': None\n",
      "\n",
      "Predicted word after 'is': is\n",
      "\n",
      "Predicted word after 'Forbes': None\n"
     ]
    }
   ],
   "source": [
    "# For the next text, perform the following actions\n",
    "text = \"The president of the U.S.A., Donald Trump, is 1.9m high and 78 years old. Forbes Magazine has assessed his wealth, currently estimating it at $5.5 billion as of mid-February 2025.\"\n",
    "\n",
    "# (1 point) 1 - Use NLTK to split the sentences \n",
    "print(\"Splitted sentences: \", nltk.word_tokenize(text))\n",
    "\n",
    "# (2 points) 2 - Convert with regex the acronym U.S.A. to USA, the number 1.9m to 190 centimeters or any other number of a height like that (e.g. 1.75m to 175 centimeters), and \"$5.5 billion\" to five point five billion.\n",
    "import re\n",
    "\n",
    "# Replace a dot immediately preceding a letter/digit with just that letter/digit.\n",
    "text2 = re.sub(r'\\.(?=[A-Za-z])', '', text)\n",
    "text3 = re.sub(r'\\.(?=[,\\s])', '', text2)\n",
    "#text4 = re.sub(r'\\.+[1-9]+m', r'\\1 0 centimeters', text2)\n",
    "text4 = re.sub(r'(\\d+)\\.(\\d{2})m\\b', r'\\g<1>\\g<2> centimeters', text3)\n",
    "text5 = re.sub(r'(\\d+)\\.(\\d)m\\b', r'\\g<1>\\g<2>0 centimeters', text4)\n",
    "digit_words = {'0': 'zero', '1': 'one', '2': 'two', '3': 'three', '4': 'four',\n",
    "               '5': 'five', '6': 'six', '7': 'seven', '8': 'eight', '9': 'nine'}\n",
    "# Step 1: Remove the $ and extra space, so \"$5.5 billion\" becomes \"5.5 billion\"\n",
    "text6 = re.sub(r'\\$(\\d+(?:\\.\\d+)?)\\s*(billion|million|thousand)', r'\\1 \\2', text5, flags=re.IGNORECASE)\n",
    "\n",
    "# Step 2: Convert digits and the period in the number to words.\n",
    "# Replace each digit with its word form.\n",
    "for digit, word in digit_words.items():\n",
    "    text6 = re.sub(digit, word, text6)\n",
    "# Replace the decimal point with \" point \"\n",
    "text6 = re.sub(r'\\.', ' point ', text6)\n",
    "\n",
    "print(\"\\nThe converted text is: \", text6)\n",
    "\n",
    "\n",
    "# (1 point) 3 - Convert to lowercase except the proper nouns that must keep the original case. For the multiword proper names convert them to an unique word joining the two word with underscoere (Juan Fernández -> Juan_Fernández).\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "normalized_tokens = [token.lower() for token in tokens]\n",
    "clean_text = ' '.join(normalized_tokens)\n",
    "print(\"\\nLower case is: \", clean_text)\n",
    "# (1 point) 4 - Tokenize the text (use the tool you prefer). \n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"\\nTokens are: \", tokens)\n",
    "# (1 point) 5 - Remove the stopwords (use the tool you prefer). \n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "word_tokens = word_tokenize(text)\n",
    "# converts the words in word_tokens to lower case and then checks whether \n",
    "#they are present in stop_words or not\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "#with no lower case conversion\n",
    "filtered_sentence = []\n",
    " \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "clean_text = ' '.join(filtered_sentence)\n",
    "print(\"\\nText without stopwords: \", clean_text)\n",
    "# (1 point) 6 - Create bigrams with pure python.\n",
    "# Tokenize the text (text6) using TreebankWordTokenizer\n",
    "tokens = tokenizer.tokenize(text6)\n",
    "\n",
    "# Create a list to store the bigrams\n",
    "bigram_list = []\n",
    "\n",
    "# Build bigrams from the token list\n",
    "for i in range(len(tokens) - 1):\n",
    "    bigram = (tokens[i], tokens[i + 1])\n",
    "    bigram_list.append(bigram)\n",
    "\n",
    "# Now bigram_list contains all the bigrams\n",
    "print(\"\\nBigrams:\", bigram_list)\n",
    "\n",
    "# (2 point) 7 - Create a language model that predict the next word using bigrams. Please explain in the code how you made the calculations.\n",
    "# Go through the list of tokens and count how many times each word is followed by another.\n",
    "\n",
    "# Import needed tools\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Our sample text (this could be any text you want to model)\n",
    "text6 = (\"The president president president is is after president after president\")\n",
    "\n",
    "# Step 1: Tokenize the text into individual words\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text6)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Step 2: Build a model of word pairs (bigrams).\n",
    "# We use a dictionary where each key is a word, and the value is a counter (a tally)\n",
    "# of words that come immediately after it.\n",
    "bigram_model = defaultdict(Counter)\n",
    "\n",
    "# Go through the list of tokens and count how many times each word is followed by another.\n",
    "for i in range(len(tokens) - 1):\n",
    "    current_word = tokens[i]\n",
    "    next_word = tokens[i + 1]\n",
    "    bigram_model[current_word][next_word] += 1\n",
    "\n",
    "# For clarity, print the bigram model\n",
    "print(\"\\nBigram model (each word and the words that follow it with counts):\")\n",
    "for word, next_words in bigram_model.items():\n",
    "    print(f\"{word} -> {dict(next_words)}\")\n",
    "\n",
    "# Step 3: Create a simple function to predict the next word.\n",
    "# Given a word, this function looks up the most common word that comes after it.\n",
    "def predict_next_word(word):\n",
    "    # Check if the word exists in our model\n",
    "    if word in bigram_model:\n",
    "        # Get the next words and their counts, and choose the one with the highest count\n",
    "        most_common = bigram_model[word].most_common(1)\n",
    "        if most_common:\n",
    "            return most_common[0][0]\n",
    "    return None\n",
    "\n",
    "# Let's test our model by predicting the next word after some sample words.\n",
    "sample_words = [\"USA,\", \"Donald\", \"is\", \"Forbes\"]\n",
    "for word in sample_words:\n",
    "    prediction = predict_next_word(word)\n",
    "    print(f\"\\nPredicted word after '{word}': {prediction}\")\n",
    "\n",
    "# Explanation:\n",
    "# 1. We first split the text into tokens (words).\n",
    "# 2. We then create pairs of words (bigrams) and count how often each pair occurs.\n",
    "#    For example, if \"Donald\" is often followed by \"Trump\", the model will record that.\n",
    "# 3. Finally, the function 'predict_next_word' looks at the counts for a given word\n",
    "#    and picks the most frequent next word.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
