{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise after class\n",
    "\n",
    "The goal of this exercise is to explore the NLTK library using the manual or online tutorials (avoid AI usage). First, create one (or more) texts in spanish (just copy paste it somewhere). The use the NLTK library of pure Python to performe at least this action to the text(s). \n",
    "\n",
    "1. Case folding\n",
    "2. Word normalization\n",
    "3. Tokenization\n",
    "4. Stemming\n",
    "5. Lemmatization\n",
    "6. Sentence segmentation\n",
    "7. PoS Tagging\n",
    "8. Named Entity Recognition (NER)\n",
    "\n",
    "Just try to explore and understand the library. Check in the reference book and NLTK manual for the new concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['algo', 'en', 'español', 'xd', ',', 'no', 'soy', 'de', 'e.e.u.u', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "\n",
    "text = \"Algo en español xd, no soy de E.E.U.U.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "normalized_tokens = [token.lower() for token in tokens]\n",
    "\n",
    "\n",
    "print(normalized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algo\n",
      "en\n",
      "español\n",
      "xd\n",
      ",\n",
      "no\n",
      "soy\n",
      "de\n",
      "eeuu\n",
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\,'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\,'\n",
      "C:\\Users\\iagoc\\AppData\\Local\\Temp\\ipykernel_23312\\1232374691.py:4: SyntaxWarning: invalid escape sequence '\\,'\n",
      "  punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'(\\w)+.(\\w)+'\n",
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "\n",
    "for token in normalized_tokens:\n",
    "    if token not in punctuations:\n",
    "        for char in token:\n",
    "            if char in punctuations:\n",
    "                    \n",
    "                token = token.replace(char, \"\")\n",
    "    print(token)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iagoc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\iagoc/nltk_data', 'c:\\\\Users\\\\iagoc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data', 'c:\\\\Users\\\\iagoc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data', 'c:\\\\Users\\\\iagoc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\iagoc\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\iagoc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Algo', 'en', 'español', 'xd', ',', 'no', 'soy', 'de', 'E.E.U.U', '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "text = \"Algo en español xd, no soy de E.E.U.U.\"\n",
    "nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt_tab')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algo en español xd, no soy de e.e.u.u.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Inicializar el stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Probar con algunas palabras\n",
    "words = \"Algo en español xd, no soy de E.E.U.U.\"\n",
    "stemmed_words = stemmer.stem(words)\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocas : rock\n",
      "cuerpo : corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\iagoc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    " \n",
    "print(\"rocas :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"cuerpo :\", lemmatizer.lemmatize(\"corpora\"))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iagoc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Algo en español xd, no soy de E.E.U.U.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')  # Necesario para el modelo de tokenización\n",
    "\n",
    "text = \"Algo en español xd, no soy de E.E.U.U.\"\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PoS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iagoc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\iagoc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\iagoc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Algo', 'NNP'), ('en', 'CC'), ('español', 'JJ'), ('xd', 'NNP'), (',', ','), ('no', 'DT'), ('soy', 'NN'), ('de', 'IN'), ('E.E.U.U', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "text = \"Algo en español xd, no soy de E.E.U.U.\"\n",
    "words = word_tokenize(text)  # Tokenización en palabras\n",
    "pos_tags = pos_tag(words)  # Etiquetado gramatical\n",
    "\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iagoc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\iagoc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\iagoc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\iagoc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\iagoc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Algo/NNP)\n",
      "  en/CC\n",
      "  español/JJ\n",
      "  xd/NNP\n",
      "  ,/,\n",
      "  no/DT\n",
      "  soy/NN\n",
      "  de/IN\n",
      "  E.E.U.U/NNP\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded:\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "# Example sentence\n",
    "sentence = \"Algo en español xd, no soy de E.E.U.U.\"\n",
    "\n",
    "# Step 1: Tokenize the sentence\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Step 2: Tag tokens with part of speech\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "# Step 3: Perform named entity recognition\n",
    "named_entities = nltk.ne_chunk(tagged_tokens)\n",
    "\n",
    "# Display the resulting tree\n",
    "print(named_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
