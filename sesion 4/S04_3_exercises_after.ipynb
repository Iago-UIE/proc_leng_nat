{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise after class\n",
    "\n",
    "The goal of this exercise is to explore the NLTK library using the manual or online tutorials (avoid AI usage). First, create one (or more) texts in spanish (just copy paste it somewhere). The use the NLTK library of pure Python to performe at least this action to the text(s). \n",
    "\n",
    "1. Case folding\n",
    "2. Word normalization\n",
    "3. Tokenization\n",
    "4. Stemming\n",
    "5. Lemmatization\n",
    "6. Sentence segmentation\n",
    "7. PoS Tagging\n",
    "8. Named Entity Recognition (NER)\n",
    "\n",
    "Just try to explore and understand the library. Check in the reference book and NLTK manual for the new concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['algo', 'en', 'español', 'xd', ',', 'no', 'soy', 'de', 'e.e.u.u', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "\n",
    "text = \"Algo en español xd, no soy de E.E.U.U.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "normalized_tokens = [token.lower() for token in tokens]\n",
    "\n",
    "\n",
    "print(normalized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algo\n",
      "en\n",
      "español\n",
      "xd\n",
      ",\n",
      "no\n",
      "soy\n",
      "de\n",
      "eeuu\n",
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\,'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\,'\n",
      "C:\\Users\\iagoc\\AppData\\Local\\Temp\\ipykernel_46132\\1232374691.py:4: SyntaxWarning: invalid escape sequence '\\,'\n",
      "  punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'(\\w)+.(\\w)+'\n",
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "\n",
    "for token in normalized_tokens:\n",
    "    if token not in punctuations:\n",
    "        for char in token:\n",
    "            if char in punctuations:\n",
    "                    \n",
    "                token = token.replace(char, \"\")\n",
    "    print(token)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
